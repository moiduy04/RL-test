{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c6f2ca",
   "metadata": {},
   "source": [
    "Guide: \"https://youtu.be/HR8kQMTO8bk\n",
    "\n",
    "PPO paper: \"https://arxiv.org/pdf/1707.06347.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ff109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\APC\\\\miniconda3\\\\Lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccce297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa430c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d39f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy and value model\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Params: An Observation\n",
    "        # Returns: Features\n",
    "        self.feature_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Params: Features\n",
    "        # Returns: An Action - according to agent's policy\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_space_size)\n",
    "        )\n",
    "        \n",
    "        # Params: Features\n",
    "        # Returns: 1 integer denoting how valuable the current state is\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def value(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        value = self.value_layers(feat)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        policy_logits = self.policy_layers(feat)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        policy_logits = self.policy_layers(feat)\n",
    "        value = self.value_layers(feat)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c060f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Trainer\n",
    "class PPO():\n",
    "    def __init__(self,\n",
    "                 actorCritic,\n",
    "                 epsilon,\n",
    "                 target_kl_div,\n",
    "                 max_policy_iters,\n",
    "                 value_iters,\n",
    "                 policy_lr,\n",
    "                 value_lr):\n",
    "        self.ac = actorCritic\n",
    "        self.epsilon = epsilon\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_iters = max_policy_iters\n",
    "        self.value_iters = value_iters\n",
    "        \n",
    "        policy_params = list(self.ac.feature_layers.parameters()) + list(self.ac.policy_layers.parameters())       \n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "        \n",
    "        value_params  = list(self.ac.feature_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "    \n",
    "#     def change_learning_rate(policy_lr, value_lr):\n",
    "#         policy_params = list(self.ac.feature_layers.parameters()) + list(self.ac.policy_layers.parameters())       \n",
    "#         self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "        \n",
    "#         value_params  = list(self.ac.feature_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "#         self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "        \n",
    "    '''\n",
    "    PPO formula:\n",
    "        L^CPI(theta)  = E_t[ pi_theta (a_t|s_t) / pi_theta(old) (a_t|s_t) * A_hat_t]\n",
    "                      = E_t[ratio_t(theta) * A_hat_t]\n",
    "\n",
    "        L^CLIP(theta) = E_t[min( ratio_t(theta) * A_hat_t,\n",
    "                                 clip(ratio_t(theta), 1-epsilon, 1+epsilon) * A_hat_t )]\n",
    "    '''\n",
    "    def train_policy(self, obs, acts, old_log_probs, advantages):\n",
    "        for _ in range(self.max_policy_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "#           pi_theta / pi_theta(old)\n",
    "#           = e^ln(pi_theta) / e^ln(pi_theta(old)) \n",
    "#           = e^[ln(pi_theta) - ln(pi_theta(old))]\n",
    "\n",
    "            clipped_ratio = ratio.clamp(1 - self.epsilon, 1 + self.epsilon)\n",
    "\n",
    "            ratio_loss = ratio * advantages\n",
    "            clipped_loss = clipped_ratio * advantages\n",
    "\n",
    "            neg_L_CLIP = -torch.min(ratio_loss, clipped_loss).mean()\n",
    "#             We minimize (-L^CLIP) instead of maximizing (L^CLIP)\n",
    "#        cause I only know how to minimize a loss funciton using pyTorch\n",
    "\n",
    "            neg_L_CLIP.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "#           Calculates the KL_divergence, check if we've updated the policy enough already\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "\n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns - values)**2\n",
    "            value_loss = value_loss.mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c109b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    '''\n",
    "    discount_R(t) = sum_{i=0}^{inf} gamma^i * R(t+i)\n",
    "                  = R(t) + sum_{i=1}^{inf} gamma^i * R(t+i)\n",
    "                  = R(t) + gamma * discount_R(t+1)\n",
    "    '''\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[t]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def advantage_estimates(rewards, values, gamma = 0.99):\n",
    "    '''\n",
    "    A_hat_t = sum_{i=0}^{T} gamma^i delta_t+i\n",
    "            = delta_t + sum_{i=1}^{T} gamma^i delta_t+i\n",
    "            = delta_t + gamma * A_hat_t+1\n",
    "    where delta_t = R(t) + (gamma)V(s_t+1) - V(s_t)\n",
    "    '''\n",
    "    advantage = [float(rewards[-1] + gamma * 0 - values[-1])]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        delta_t = float(rewards[t] + gamma * values[t+1] - values[t])\n",
    "        advantage.append(delta_t + gamma * advantage[-1])\n",
    "    return np.array(advantage[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7debeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env, max_steps = 210):\n",
    "    \"\"\"\n",
    "    Params: model + environment\n",
    "\n",
    "    Run's an episode \n",
    "    with at most (max_steps) steps\n",
    "    and actions taken probabilistically from current (old) policy \n",
    "\n",
    "    Returns: Training data - (number_of_steps, observations)\n",
    "            + Total Rewards\n",
    "        \n",
    "    Does NOT do any training.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data = [ [], [], [], [], [] ]\n",
    "    # obs, act, reward, advantage_estimates, act_log_probs\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Act according to current policy\n",
    "        logits, val = model(torch.tensor([obs], \n",
    "                                         dtype = torch.float32, device = DEVICE))\n",
    "        \n",
    "        # Gets a categorical distribution of potential actions\n",
    "        act_distribution = Categorical(logits = logits)\n",
    "        \n",
    "        # Pick an action according to the distribution\n",
    "        act = act_distribution.sample()\n",
    "        # How likely was that action chosen\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "        \n",
    "        act = act.item()\n",
    "        val = val.item()\n",
    "        \n",
    "        nxt_obs, reward, terminated, truncated, _ = env.step(act)\n",
    "        \n",
    "        # Records data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "        \n",
    "        obs = nxt_obs\n",
    "        ep_reward += reward        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Train data filtering & formatting\n",
    "    train_data = [np.asarray(data) for data in train_data]\n",
    "    train_data[3] = advantage_estimates(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33dc0a",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "https://github.com/Farama-Foundation/Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd188e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_movement():\n",
    "    env = gym.make(\"LunarLander-v2\", render_mode=\"human\", continuous = False, enable_wind = False)\n",
    "    \n",
    "    observation, info = env.reset()\n",
    "\n",
    "    for _ in range(690):\n",
    "        observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    env.close()\n",
    "# random_movement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b14697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "_seed = 69\n",
    "torch.manual_seed(_seed)\n",
    "np.random.seed(_seed)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", continuous = False, enable_wind = False)\n",
    "print(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2bcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run an episode\n",
    "# test_train_data, test_reward = run_episode(model, env)\n",
    "# assert(np.array(test_train_data[0]).shape[1] == env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init PPO and training params\n",
    "n_episodes = 1_000\n",
    "print_frequency = int(n_episodes // 30)\n",
    "\n",
    "_alpha = 0.25\n",
    "ppo = PPO(model,\n",
    "         epsilon = 0.1,\n",
    "         target_kl_div = 0.01,\n",
    "         max_policy_iters = 30,\n",
    "         value_iters = 30,\n",
    "         policy_lr = 3e-6 * _alpha,\n",
    "         value_lr = 9e-6 * _alpha,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "ep_rewards = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    # Perform rollout\n",
    "    train_data, reward = run_episode(model, env, 400)\n",
    "#     train_data: obs, act, reward, value, act_log_probs\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "    # Policy data formatting\n",
    "    obs = torch.tensor(train_data[0][permute_idxs], \n",
    "                       dtype = torch.float32, device = DEVICE)\n",
    "    acts = torch.tensor(train_data[1][permute_idxs], \n",
    "                        dtype = torch.int32, device = DEVICE)\n",
    "    advantages = torch.tensor(train_data[3][permute_idxs], \n",
    "                              dtype = torch.float32, device = DEVICE)\n",
    "    act_log_probs = torch.tensor(train_data[4][permute_idxs], \n",
    "                                 dtype = torch.float32, device = DEVICE)\n",
    "    \n",
    "    # Value data formatting\n",
    "    returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "    returns = torch.tensor(returns,\n",
    "                           dtype = torch.float32, device = DEVICE)\n",
    "    \n",
    "    # Train model\n",
    "    ppo.train_policy(obs, acts, act_log_probs, advantages)\n",
    "    ppo.train_value(obs, returns)\n",
    "    \n",
    "    if (episode == 0) or ((episode+1) % print_frequency == 0) or (episode == n_episodes-1):\n",
    "        tqdm.write(f'Episode {episode+1} | Avg reward {np.mean(ep_rewards[-print_frequency:])}',\n",
    "                  file = sys.stderr)\n",
    "        if (np.mean(ep_rewards[-print_frequency:] == 200)):\n",
    "            print('SOLVED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518d5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: score 171.38206326050658 (998 steps)\n",
      "Episode 2: score 121.47211314354405 (998 steps)\n",
      "Episode 3: score 115.16247226875545 (998 steps)\n",
      "Episode 4: score 137.14449159891583 (998 steps)\n",
      "Episode 5: score 132.09691927663337 (998 steps)\n"
     ]
    }
   ],
   "source": [
    "def run_model(model, n_episode = 5):\n",
    "    env = gym.make(\"LunarLander-v2\", continuous = False, enable_wind = False)\n",
    "    for episode in range(n_episode):\n",
    "        score = 0\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        for steps in range(999):\n",
    "#             env.render()\n",
    "            logits, _ = model(torch.tensor([obs], \n",
    "                                             dtype = torch.float32, device = DEVICE))\n",
    "            act_distribution = Categorical(logits = logits)\n",
    "            act = act_distribution.sample().item()\n",
    "\n",
    "            nxt_obs, reward, terminated, truncated, _ = env.step(act)\n",
    "            \n",
    "            obs = nxt_obs\n",
    "            score += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        print(f'Episode {episode+1}: score {score} ({steps} steps)')\n",
    "    env.close()\n",
    "    \n",
    "run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd015db",
   "metadata": {},
   "source": [
    "PPO formula:\\\n",
    "\n",
    "    L^CPI(theta)  = E_t[ pi_theta (a_t|s_t) / pi_theta(old) (a_t|s_t) * A_hat_t]\n",
    "                  = E_t[ratio_t(theta) * A_hat_t]\n",
    "    \n",
    "    L^CLIP(theta) = E_t[min( ratio_t(theta) * A_hat_t,\n",
    "                             clip(ratio_t(theta), 1-epsilon, 1+epsilon) * A_hat_t )]\n",
    "\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e2326",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0890d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"SavedModels/2023-06-18-1953.pth\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f49bf91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOAD_PATH = \"SavedModels/2023-06-18.pth\" # <-- this is the best model\n",
    "# model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "# model.load_state_dict(torch.load(LOAD_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1292b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
