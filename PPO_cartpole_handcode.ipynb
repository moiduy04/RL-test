{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da279838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide: https://youtu.be/HR8kQMTO8bk\n",
    "# PPO paper: https://arxiv.org/pdf/1707.06347.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccce297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa430c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d39f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy and value model\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Params: An Observation\n",
    "        # Returns: Features\n",
    "        self.feature_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Params: Features\n",
    "        # Returns: An Action - according to agent's policy\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size)\n",
    "        )\n",
    "        \n",
    "        # Params: Features\n",
    "        # Returns: 1 integer denoting how valuable the current state is\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def value(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        value = self.value_layers(feat)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        policy_logits = self.policy_layers(feat)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        feat = self.feature_layers(obs)\n",
    "        policy_logits = self.policy_layers(feat)\n",
    "        value = self.value_layers(feat)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c060f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Trainer\n",
    "class PPO():\n",
    "    def __init__(self,\n",
    "                 actorCritic,\n",
    "                 epsilon = 0.2,\n",
    "                 target_kl_div = 0.01,\n",
    "                 max_policy_iters = 69,\n",
    "                 value_iters = 69,\n",
    "                 policy_lr = 3e-4,\n",
    "                 value_lr = 3e-4):\n",
    "        self.ac = actorCritic\n",
    "        self.epsilon = epsilon\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_iters = max_policy_iters\n",
    "        self.value_iters = value_iters\n",
    "        \n",
    "        policy_params = list(self.ac.feature_layers.parameters()) + list(self.ac.policy_layers.parameters())       \n",
    "        self.policy_optim = optim.Adam(policy_params, lr = policy_lr)\n",
    "        \n",
    "        value_params  = list(self.ac.feature_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr = value_lr)\n",
    "        \n",
    "    '''\n",
    "    PPO formula:\n",
    "        L^CPI(theta)  = E_t[ pi_theta (a_t|s_t) / pi_theta(old) (a_t|s_t) * A_hat_t]\n",
    "                      = E_t[ratio_t(theta) * A_hat_t]\n",
    "\n",
    "        L^CLIP(theta) = E_t[min( ratio_t(theta) * A_hat_t,\n",
    "                                 clip(ratio_t(theta), 1-epsilon, 1+epsilon) * A_hat_t )]\n",
    "    '''\n",
    "    def train_policy(self, obs, acts, old_log_probs, advantages):\n",
    "        for _ in range(self.max_policy_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits = new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "#           pi_theta / pi_theta(old)\n",
    "#           = e^ln(pi_theta) / e^ln(pi_theta(old)) \n",
    "#           = e^[ln(pi_theta) - ln(pi_theta(old))]\n",
    "\n",
    "            clipped_ratio = ratio.clamp(1 - self.epsilon, 1 + self.epsilon)\n",
    "\n",
    "            ratio_loss = ratio * advantages\n",
    "            clipped_loss = clipped_ratio * advantages\n",
    "\n",
    "            neg_L_CLIP = -torch.min(ratio_loss, clipped_loss).mean()\n",
    "#             We minimize (-L^CLIP) instead of maximizing (L^CLIP)\n",
    "#        cause I only know how to minimize a loss funciton using pyTorch\n",
    "\n",
    "            neg_L_CLIP.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "#           Calculates the KL_divergence, check if we've updated the policy enough already\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "\n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns - values)**2\n",
    "            value_loss = value_loss.mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c109b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def discount_rewards(rewards, gamma = 0.99):\n",
    "    '''\n",
    "    discount_R(t) = sum_{i=0}^{inf} gamma^i * R(t+i)\n",
    "                  = R(t) + sum_{i=1}^{inf} gamma^i * R(t+i)\n",
    "                  = R(t) + gamma * discount_R(t+1)\n",
    "    '''\n",
    "    \n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[t]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def advantage_estimates(rewards, values, gamma = 0.99):\n",
    "    '''\n",
    "    A_hat_t = sum_{i=0}^{T} gamma^i delta_t+i\n",
    "            = delta_t + sum_{i=1}^{T} gamma^i delta_t+i\n",
    "            = delta_t + gamma * A_hat_t+1\n",
    "    where delta_t = R(t) + (gamma)V(s_t+1) - V(s_t)\n",
    "    '''\n",
    "    advantage = [float(rewards[-1] + gamma * 0 - values[-1])]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        delta_t = float(rewards[t] + gamma * values[t+1] - values[t])\n",
    "        advantage.append(delta_t + gamma * advantage[-1])\n",
    "    return np.array(advantage[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7debeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env, max_steps = 201):\n",
    "    \"\"\"\n",
    "    Params: model + environment\n",
    "\n",
    "    Run's an episode \n",
    "    with at most (max_steps) steps\n",
    "    and actions taken probabilistically from current (old) policy \n",
    "\n",
    "    Returns: Training data - (number_of_steps, observations)\n",
    "            + Total Rewards\n",
    "        \n",
    "    Does NOT do any training.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data = [ [], [], [], [], [] ]\n",
    "    # obs, act, reward, advantage_estimates, act_log_probs\n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Act according to current policy\n",
    "        logits, val = model(torch.tensor([obs], \n",
    "                                         dtype = torch.float32, device = DEVICE))\n",
    "        \n",
    "        # Gets a categorical distribution of potential actions\n",
    "        act_distribution = Categorical(logits = logits)\n",
    "        \n",
    "        # Pick an action according to the distribution\n",
    "        act = act_distribution.sample()\n",
    "        # How likely was that action chosen\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "        \n",
    "        act = act.item()\n",
    "        val = val.item()\n",
    "        \n",
    "        nxt_obs, reward, done, _ = env.step(act)\n",
    "        \n",
    "        # Records data for training\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "        \n",
    "        obs = nxt_obs\n",
    "        ep_reward += reward        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Train data filtering & formatting\n",
    "    train_data = [np.asarray(data) for data in train_data]\n",
    "    train_data[3] = advantage_estimates(train_data[2], train_data[3])\n",
    "    \n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b14697",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#   Note:\n",
    "# CartPole's activation space is a Descrete Value, \n",
    "# as such 'env.action_space.shape' returns '()'\n",
    "\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2bcd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\APC\\AppData\\Local\\Temp\\ipykernel_9124\\372390914.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  logits, val = model(torch.tensor([obs],\n"
     ]
    }
   ],
   "source": [
    "# Test run an episode\n",
    "train_data, reward = run_episode(model, env)\n",
    "# assert(np.array(train_data[0]).shape[1] == env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init PPO and params\n",
    "n_episodes = 100\n",
    "print_frequency = n_episodes // 5\n",
    "np.random.seed(69)\n",
    "\n",
    "ppo = PPO(model,\n",
    "         epsilon = 0.2,\n",
    "         target_kl_div = 0.02,\n",
    "         max_policy_iters = 40,\n",
    "         value_iters = 40,\n",
    "         policy_lr = 3,\n",
    "         value_lr = 3\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77f2cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Avg reward 12.0\n",
      "Episode 20 | Avg reward 12.0\n",
      "Episode 40 | Avg reward 12.0\n",
      "Episode 60 | Avg reward 12.0\n",
      "Episode 80 | Avg reward 12.0\n",
      "Episode 100 | Avg reward 12.0\n"
     ]
    }
   ],
   "source": [
    "## Training loop\n",
    "ep_rewards = []\n",
    "for episode in range(n_episodes):\n",
    "    # Perform rollout\n",
    "    train_data, returns = run_episode(model, env)\n",
    "#     train_data: obs, act, reward, value, act_log_probs\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "    # Policy data formatting\n",
    "    obs = torch.tensor(train_data[0][permute_idxs], \n",
    "                       dtype = torch.float32, device = DEVICE)\n",
    "    acts = torch.tensor(train_data[1][permute_idxs], \n",
    "                        dtype = torch.int32, device = DEVICE)\n",
    "    advantages = torch.tensor(train_data[3][permute_idxs], \n",
    "                              dtype = torch.float32, device = DEVICE)\n",
    "    act_log_probs = torch.tensor(train_data[4][permute_idxs], \n",
    "                                 dtype = torch.float32, device = DEVICE)\n",
    "    \n",
    "    # Value data formatting\n",
    "    returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "    returns = torch.tensor(returns,\n",
    "                           dtype = torch.float32, device = DEVICE)\n",
    "    \n",
    "    # Train model\n",
    "    ppo.train_policy(obs, acts, act_log_probs, advantages)\n",
    "    ppo.train_value(obs, returns)\n",
    "    \n",
    "    if (episode == 0) or ((episode+1) % print_frequency == 0) or (episode == n_episodes-1):\n",
    "        print(f'Episode {episode+1} | Avg reward {np.mean(ep_rewards[-print_frequency:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66f200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014c54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PPO formula:\n",
    "    L^CPI(theta)  = E_t[ pi_theta (a_t|s_t) / pi_theta(old) (a_t|s_t) * A_hat_t]\n",
    "                  = E_t[ratio_t(theta) * A_hat_t]\n",
    "    \n",
    "    L^CLIP(theta) = E_t[min( ratio_t(theta) * A_hat_t,\n",
    "                             clip(ratio_t(theta), 1-epsilon, 1+epsilon) * A_hat_t )]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298ef89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
